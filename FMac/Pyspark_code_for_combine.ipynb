{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys   \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g')])\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"beginner\") \\\n",
    "   .config(conf=conf) \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'beginner'),\n",
       " ('spark.driver.port', '43663'),\n",
       " ('spark.cores.max', '3'),\n",
       " ('spark.driver.host', '192.168.0.141'),\n",
       " ('spark.app.id', 'local-1530794262852'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.executor.cores', '3'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNAN(df):\n",
    "    df = df.fillna('X', subset=['flag_fthb', 'occpy_sts','channel','ppmt_pnlty',\n",
    "                           'prod_type','loan_purpose','flag_sc'])\n",
    "    df = df.fillna('999',subset=['mi_pct','cltv','dti','ltv'])\n",
    "    df = df.fillna('9999',subset=['fico'])\n",
    "    df = df.fillna('0',subset=['cd_msa'])\n",
    "    df = df.fillna('99',subset=['cnt_units','cnt_borr'])\n",
    "    df = df.fillna('XX',subset=['prop_type'])\n",
    "    df = df.fillna('999999',subset=['zipcode']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changedatatype(df):\n",
    "    #Change the data types for all column\n",
    "    for c in ['fico','cd_msa','mi_pct','cnt_units','cltv','dti','orig_upb','ltv','orig_loan_term']:\n",
    "        df = df.withColumn(c,col(c).cast(\"double\"))\n",
    "    for c in ['flag_sc','flag_fthb','dt_first_pi','dt_matr','cnt_borr','occpy_sts','channel','ppmt_pnlty','zipcode','servicer_name','id_loan','loan_purpose','seller_name']:\n",
    "        df = df.withColumn(c,col(c).cast(\"String\"))\n",
    "    #     df[ = df[['flag_sc','flag_fthb','cnt_borr','occpy_sts','channel','ppmt_pnlty','zipcode','servicer_name','id_loan','loan_purpose','seller_name']].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bins(df,in_col,out_col,splits):\n",
    "    bucketizer = Bucketizer(splits=splits,inputCol=in_col, outputCol=out_col)\n",
    "    df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "    t1 = {}\n",
    "    for i in range(len(splits)-1):\n",
    "        t1[float(i)] = \"[\"+str(splits[i])+\",\"+str(splits[i+1])+\")\"\n",
    "    # t = {0.0:\"[0,600)\", 1.0: \"[600,680)\", 2.0 : \"[680,720)\", 3.0: \"[720,760)\", 4.0: \"[760,780)\", 5.0: \"[780,850)\", 6.0: \"[850,900)\", 7.0: \"[900,9999)\",}\n",
    "    udf_foo = udf(lambda x: t1[x], StringType())\n",
    "    df = df.withColumn(out_col, udf_foo(out_col))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOriginationCombined(filepath):\n",
    "    #print(str)\n",
    "    \n",
    "    writeHeader1 = True\n",
    "    if \"sample\" in filepath:\n",
    "        filename= \"SampleOriginationCombined\"\n",
    "    else:\n",
    "        filename= \"HistoricalOriginationCombined\"\n",
    "#     if os.path.exists(filename):\n",
    "#         shutil.rmtree(filename)\n",
    "    files = glob.glob(filepath)\n",
    "    F1 = udf(lambda x:x[:4]+\"-\"+x[4:]+\"-1\", StringType())\n",
    "    F2 = udf(lambda x : '19'+x[2:4] if x[2:4]=='99' else '20'+x[2:4])\n",
    "    t1 = time.time()\n",
    "    df = spark.read.options(header=False,inferSchema=False).csv(files,sep='|',)\n",
    "    df = df.toDF('fico','dt_first_pi','flag_fthb','dt_matr','cd_msa',\"mi_pct\",'cnt_units',\n",
    "                     'occpy_sts','cltv','dti','orig_upb','ltv','int_rt','channel','ppmt_pnlty',\n",
    "                     'prod_type','st', 'prop_type','zipcode','id_loan','loan_purpose', \n",
    "                     'orig_loan_term','cnt_borr','seller_name','servicer_name','flag_sc')\n",
    "    df = fillNAN(df)\n",
    "    df = changedatatype(df)\n",
    "    df = df.withColumn(\"flag_fthb\",when(df.flag_fthb=='9','X').otherwise(df.flag_fthb))\n",
    "    # F1 = udf(lambda x:x[:4]+\"-\"+x[4:]+\"-1\", StringType())\n",
    "    # df.withColumn(\"dt_first_pi\", F1(col(\"dt_first_pi\"))).select(\"dt_first_pi\").show()\n",
    "    for c in ['dt_first_pi','dt_matr']:\n",
    "        df = df.withColumn(c,F1(col(c)).cast(DateType()).alias(c))\n",
    "    df = make_bins(df,\"fico\",\"fico_bins\",[0,600,680,720,760,780,850,900,9999])\n",
    "    df = make_bins(df,\"cltv\",\"cltv_bins\",[0,6,50,70,80,90,110,150,200,999])\n",
    "    df = make_bins(df,\"dti\",\"dti_bins\",[0,27,36,46,65,999])\n",
    "    df = make_bins(df,\"ltv\",\"ltv_bins\",[6,50,70,80,90,105,999])\n",
    "    df = make_bins(df,\"mi_pct\",\"mi_pct_bins\",[0,20,30,40,55,999])\n",
    "    # F2 = udf(lambda x : '19'+x[2:4] if x[2:4]=='99' else '20'+x[2:4])\n",
    "    df = df.withColumn(\"Year\",F2(\"id_loan\"))\n",
    "        \n",
    "#     df.toPandas().to_csv(file, mode='a', header=writeHeader1,index=False,encoding='utf-8')\n",
    "    df.repartition(1).write.csv(filename,header=True,sep=',')\n",
    "    shutil.move(glob.glob(os.getcwd()+\"/\"+filename+\"/*.csv\")[0],os.getcwd()+\"/\"+filename+\".csv\")\n",
    "    shutil.rmtree(filename)\n",
    "    t2 = time.time()\n",
    "    print(\"Total time with pyspark : {}\".format(t2-t1))\n",
    "    return filename+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ts = time.time()\n",
    "    foldername= 'SampleInputFiles'\n",
    "    \n",
    "    sampleOrigFiles=str(os.getcwd())+\"/\"+foldername+\"/sample_orig_*.txt\"\n",
    "#     samplePerfFiles=str(os.getcwd())+\"/\"+foldername+\"/sample_svcg_*.txt\"\n",
    "    \n",
    "    orig_file = createOriginationCombined(sampleOrigFiles)\n",
    "    df11 = spark.read.options(header = True,inferSchema=True).csv(orig_file)\n",
    "    print(df11.count())\n",
    "    \n",
    "#     per_file = createPerformanceCombined(samplePerfFiles)\n",
    "    \n",
    "#     orig_df = pd.read_csv(orig_file)\n",
    "#     per_df = pd.read_csv(per_file,dtype={'delq_sts':'str'})\n",
    "#     combined_df = orig_df.merge(per_df,on='id_loan')\n",
    "#     combined_df.to_csv('combined_SF_smaple_data.csv', encoding='utf-8', index=False)\n",
    "    \n",
    "#     com1_df = pd.read_csv('combined_SF_smaple_data.csv')\n",
    "\n",
    "#     orig_summary_statistic1 = com1_df.groupby(\"Year\").apply(orig_summary_statistics).round(1)\n",
    "#     performance_summary_statistic1 = com1_df.groupby(\"Year\").apply(performance_summary_statistics).round(1)\n",
    "\n",
    "#     orig_summary_statistic1.to_csv(\"sample_SF_orig_summary_Statistics.csv\",index=False)\n",
    "#     performance_summary_statistic1.to_csv(\"sample_SF_performance_summary_Statistics.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time with pyspark : 14.63238263130188\n",
      "912500\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = spark.read.options(header = True,inferSchema=True).csv('/home/dangar/Desktop/Pyspark tutorial/SampleOriginationCombined.csv/part-00000-9d3ce88a-2e60-4bb4-9f70-8e43b543735d-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fico: double (nullable = true)\n",
      " |-- dt_first_pi: timestamp (nullable = true)\n",
      " |-- flag_fthb: string (nullable = true)\n",
      " |-- dt_matr: timestamp (nullable = true)\n",
      " |-- cd_msa: double (nullable = true)\n",
      " |-- mi_pct: double (nullable = true)\n",
      " |-- cnt_units: double (nullable = true)\n",
      " |-- occpy_sts: string (nullable = true)\n",
      " |-- cltv: double (nullable = true)\n",
      " |-- dti: double (nullable = true)\n",
      " |-- orig_upb: double (nullable = true)\n",
      " |-- ltv: double (nullable = true)\n",
      " |-- int_rt: double (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- ppmt_pnlty: string (nullable = true)\n",
      " |-- prod_type: string (nullable = true)\n",
      " |-- st: string (nullable = true)\n",
      " |-- prop_type: string (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- id_loan: string (nullable = true)\n",
      " |-- loan_purpose: string (nullable = true)\n",
      " |-- orig_loan_term: double (nullable = true)\n",
      " |-- cnt_borr: integer (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      " |-- servicer_name: string (nullable = true)\n",
      " |-- flag_sc: string (nullable = true)\n",
      " |-- fico_bins: string (nullable = true)\n",
      " |-- cltv_bins: string (nullable = true)\n",
      " |-- dti_bins: string (nullable = true)\n",
      " |-- ltv_bins: string (nullable = true)\n",
      " |-- mi_pct_bins: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNA(df):\n",
    "    df['delq_sts'] = df['delq_sts'].fillna('XX')\n",
    "    df['loan_age'] = df['loan_age'].fillna(999)\n",
    "    df['mths_remng'] = df['mths_remng'].fillna('XX')\n",
    "    df['repch_flag']=df['repch_flag'].fillna('X')\n",
    "    df['flag_mod']=df['flag_mod'].fillna('X')\n",
    "    df['cd_zero_bal']=df['cd_zero_bal'].fillna('00')\n",
    "    df['dt_zero_bal']=df['dt_zero_bal'].fillna('189901')\n",
    "    df['non_int_brng_upb']=df['non_int_brng_upb'].fillna(0)\n",
    "    df['dt_lst_pi']=df['dt_lst_pi'].fillna('189901')\n",
    "    df['mi_recoveries']=df['mi_recoveries'].fillna(0)\n",
    "    df['net_sale_proceeds']=df['net_sale_proceeds'].fillna('0')\n",
    "    df['non_mi_recoveries']=df['non_mi_recoveries'].fillna(0)\n",
    "    df['expenses']=df['expenses'].fillna(0)\n",
    "    df['legal_costs']=df['legal_costs'].fillna(0)\n",
    "    df['maint_pres_costs']=df['maint_pres_costs'].fillna(0)\n",
    "    df['taxes_ins_costs']=df['taxes_ins_costs'].fillna(0)\n",
    "    df['misc_costs']=df['misc_costs'].fillna(0)\n",
    "    df['actual_loss']=df['actual_loss'].fillna(0)\n",
    "    df['modcost']=df['modcost'].fillna(0)\n",
    "    df['stepmod_ind']=df['stepmod_ind'].fillna('X')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chnge_code_zero(x):\n",
    "    if x=='00':\n",
    "        return 'C'\n",
    "    elif x=='01':\n",
    "        return 'P'\n",
    "    elif x=='06':\n",
    "        return 'R'\n",
    "    elif x=='03':\n",
    "        return 'S'\n",
    "    elif x=='09':\n",
    "        return 'F'\n",
    "def chnge_delinquecy(x):\n",
    "    if x=='0':\n",
    "        return 'C'\n",
    "    elif x not in list(map(str,list(range(1,9))))+['R']:\n",
    "        return '9+'\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "writeHeader2=True\n",
    "perf_df = pd.read_csv(\"SampleInputFiles/sample_svcg_2011.txt\",sep=\"|\",header=None,skipinitialspace=True,dtype='unicode')\n",
    "perf_df.columns =['id_loan','svcg_cycle','current_upb','delq_sts','loan_age','mths_remng', 'repch_flag',\n",
    "                  'flag_mod','cd_zero_bal', 'dt_zero_bal','current_int_rt','non_int_brng_upb','dt_lst_pi',\n",
    "                  'mi_recoveries','net_sale_proceeds','non_mi_recoveries','expenses', 'legal_costs', \n",
    "                  'maint_pres_costs','taxes_ins_costs','misc_costs','actual_loss', 'modcost','stepmod_ind']\n",
    "#             perf_df['delq_sts'] = [ 999 if x=='R' else x for x in (perf_df['delq_sts'].apply(lambda x: x))]\n",
    "#             perf_df['delq_sts'] = [ 0 if x=='XX' else x for x in (perf_df['delq_sts'].apply(lambda x: x))]\n",
    "perf_df.loc[(perf_df.net_sale_proceeds=='U')|(perf_df.net_sale_proceeds=='C'),'net_sale_proceeds'] = '0'\n",
    "#             perf_df['net_sale_proceeds'] = [ '0.0' if x=='C' else x for x in (perf_df['net_sale_proceeds'].apply(lambda x: x))]\n",
    "\n",
    "#             perf_df.cd_zero_bal = perf_df.cd_zero_bal.apply(lambda x : chnge_code_zero(x))\n",
    "\n",
    "perf_df = fillNA(perf_df)\n",
    "perf_df = changedtype(perf_df)\n",
    "perf_df.cd_zero_bal = perf_df.cd_zero_bal.apply(lambda x : chnge_code_zero(x))\n",
    "\n",
    "ve =perf_df.drop(perf_df[(perf_df.cd_zero_bal=='S')|(perf_df.cd_zero_bal=='F')].index)\n",
    "h = ve.groupby(by='id_loan').last().reset_index()\n",
    "defauled_upb = h.loc[h.id_loan.isin(perf_df[(perf_df.cd_zero_bal=='S')|(perf_df.cd_zero_bal=='F')].id_loan.values),['id_loan','current_upb']]\n",
    "\n",
    "ve1 =perf_df.drop(perf_df[(perf_df.cd_zero_bal=='P')|(perf_df.cd_zero_bal=='R')].index)\n",
    "h1 = ve1.groupby(by='id_loan').last().reset_index()\n",
    "prepaid_upb = h1.loc[h1.id_loan.isin(perf_df[(perf_df.cd_zero_bal=='P')|(perf_df.cd_zero_bal=='R')].id_loan.values),['id_loan','current_upb']]\n",
    "\n",
    "lpi = perf_df.loc[(perf_df.delq_sts=='0'),['id_loan','svcg_cycle']]\n",
    "lpi =lpi.groupby(by='id_loan').last().reset_index()\n",
    "delq_sts_180 = perf_df.loc[(perf_df.delq_sts=='6'),['id_loan','svcg_cycle','current_upb']]\n",
    "delq_sts_180 = delq_sts_180.groupby(by='id_loan').last().reset_index()\n",
    "\n",
    "perf_df = perf_df.groupby(by='id_loan').last().reset_index()\n",
    "\n",
    "perf_df['delq_sts_180_date'] = '189901'\n",
    "perf_df['last_payment_date'] = '189901'\n",
    "perf_df['delq_sts_180_upb'] = 0\n",
    "perf_df['defaulted_upb'] = 0\n",
    "perf_df['prepaid_upb'] = 0\n",
    "perf_df.loc[perf_df.id_loan.isin(delq_sts_180.id_loan.values),'delq_sts_180_date'] = delq_sts_180.svcg_cycle.values\n",
    "perf_df.loc[perf_df.id_loan.isin(delq_sts_180.id_loan.values),'delq_sts_180_upb'] = delq_sts_180.current_upb.values\n",
    "perf_df.loc[perf_df.id_loan.isin(lpi.id_loan.values),'last_payment_date'] = lpi.svcg_cycle.values\n",
    "perf_df.loc[perf_df.id_loan.isin(defauled_upb.id_loan.values),'defaulted_upb'] = defauled_upb.current_upb.values\n",
    "perf_df.loc[perf_df.id_loan.isin(prepaid_upb.id_loan.values),'prepaid_upb'] = prepaid_upb.current_upb.values\n",
    "\n",
    "#             perf_df = perf_df.drop(perf_df[perf_df.cd_zero_bal=='06'].index)\n",
    "#             perf_df = perf_df.drop('repch_flag',axis=1)\n",
    "\n",
    "perf_df.dt_lst_pi = pd.to_datetime(perf_df.dt_lst_pi.astype('str').apply(lambda x: x[:4] +'/'+x[4:]))\n",
    "perf_df.dt_zero_bal = pd.to_datetime(perf_df.dt_zero_bal.astype('str').apply(lambda x: x[:4] +'/'+x[4:]))\n",
    "perf_df.delq_sts_180_date = pd.to_datetime(perf_df.delq_sts_180_date.astype('str').apply(lambda x: x[:4] +'/'+x[4:]))\n",
    "perf_df.last_payment_date = pd.to_datetime(perf_df.last_payment_date.astype('str').apply(lambda x: x[:4] +'/'+x[4:]))\n",
    "\n",
    "perf_df['GT_90_days_delinquecy'] = perf_df.delq_sts.values\n",
    "perf_df['GT_90_days_delinquecy'] = perf_df['GT_90_days_delinquecy'].apply(lambda x: 0 if (x=='0') | (x=='1') | (x=='2')|(x=='XX')  else 1)\n",
    "perf_df['default'] = perf_df.cd_zero_bal.values\n",
    "perf_df['default'] = perf_df['default'].apply(lambda x: 1 if (x=='S') | (x=='F') else 0)\n",
    "\n",
    "perf_df['prepayment']=0\n",
    "perf_df.loc[(perf_df.cd_zero_bal=='P')&(perf_df.mths_remng!=0),'prepayment'] = 1 \n",
    "#             de = perf_df[perf_df.default==1]\n",
    "#             months_delinquecny = (pd.to_datetime(de.dt_zero_bal.values).year - pd.to_datetime(de.last_payment_date.values).year)*12 + (pd.to_datetime(de.dt_zero_bal.values).month - pd.to_datetime(de.last_payment_date.values).month)\n",
    "\n",
    "perf_df['lpi2zero'] = 0\n",
    "perf_df['delinquent_interest'] = 0\n",
    "perf_df['net_loss'] = perf_df.actual_loss.copy()\n",
    "perf_df['loss_severity'] = 0\n",
    "\n",
    "c = perf_df[(perf_df.dt_lst_pi!='1899-01-01')&(perf_df.dt_zero_bal!='1899-01-01')]\n",
    "if c.shape[0]>0:\n",
    "    o = (pd.to_datetime(c.dt_zero_bal.values).year - pd.to_datetime(c.dt_lst_pi.values).year)*12 + (pd.to_datetime(c.dt_zero_bal.values).month - pd.to_datetime(c.dt_lst_pi.values).month)\n",
    "\n",
    "    perf_df.loc[(perf_df.dt_lst_pi!='1899-01-01')&(perf_df.dt_zero_bal!='1899-01-01'),'lpi2zero'] = o\n",
    "\n",
    "    de_i = perf_df.loc[(perf_df.lpi2zero!=0)&(perf_df.default==1)] \n",
    "    perf_df.loc[(perf_df.lpi2zero!=0)&(perf_df.default==1),'delinquent_interest'] = (de_i.lpi2zero) * (de_i.defaulted_upb - de_i.non_int_brng_upb) * (de_i.current_int_rt - 0.35) / 1200\n",
    "\n",
    "perf_df['total_proceeds'] = perf_df[['mi_recoveries','net_sale_proceeds', 'non_mi_recoveries']].sum(axis=1)\n",
    "\n",
    "if perf_df[perf_df.actual_loss!=0].shape[0]>0:\n",
    "    perf_df.loc[(perf_df.actual_loss!=0),'net_loss'] = perf_df.loc[(perf_df.actual_loss!=0),['actual_loss','modcost']].T.apply(lambda x: x[0]-x[1])\n",
    "\n",
    "perf_df.loc[perf_df.net_loss!=0,'loss_severity'] = perf_df.loc[perf_df.net_loss!=0,['defaulted_upb','net_loss']].T.apply(lambda x: x[1]/x[0])\n",
    "\n",
    "perf_df.delq_sts = perf_df.delq_sts.apply(lambda x : chnge_delinquecy(x))\n",
    "# perf_df.to_csv(file, mode='a', header=writeHeader2,index=False,encoding='utf-8')\n",
    "# writeHeader2=False\n",
    "t2 = time.time()\n",
    "print(\"Total time with pandas : {}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNA(df):\n",
    "    df = df.fillna('X', subset=['repch_flag','flag_mod','stepmod_ind'])\n",
    "    df = df.fillna('999',subset=['loan_age'])\n",
    "    df = df.fillna('0',subset=['non_int_brng_upb','mi_recoveries','net_sale_proceeds',\n",
    "                               'non_mi_recoveries','expenses','legal_costs','maint_pres_costs',\n",
    "                               'taxes_ins_costs','misc_costs','actual_loss','modcost'])\n",
    "    df = df.fillna('99',subset=['cnt_units'])\n",
    "    df = df.fillna('XX',subset=['delq_sts','mths_remng'])\n",
    "    df = df.fillna('189901',subset=['dt_zero_bal','dt_lst_pi'])\n",
    "    df = df.fillna('00',subset=['cd_zero_bal']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changedtype(df):\n",
    "    #Change the data types for all column\n",
    "    df[['current_upb','loan_age','mths_remng','current_int_rt','non_int_brng_upb','mi_recoveries','net_sale_proceeds','non_mi_recoveries','expenses', 'legal_costs',\n",
    "    'maint_pres_costs','taxes_ins_costs','misc_costs','actual_loss', 'modcost']] = df[['current_upb','loan_age','mths_remng','current_int_rt','non_int_brng_upb','mi_recoveries','net_sale_proceeds','non_mi_recoveries','expenses', 'legal_costs',\n",
    "    'maint_pres_costs','taxes_ins_costs','misc_costs','actual_loss', 'modcost']].astype('float64')\n",
    "    df[['id_loan','svcg_cycle','delq_sts','repch_flag','flag_mod', 'cd_zero_bal']] = df[['id_loan','svcg_cycle','delq_sts','repch_flag','flag_mod', 'cd_zero_bal']].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changedtype(df):\n",
    "    #Change the data types for all column\n",
    "    for c in ['current_upb','loan_age','mths_remng','current_int_rt','non_int_brng_upb','mi_recoveries','net_sale_proceeds','non_mi_recoveries','expenses', 'legal_costs',\n",
    "    'maint_pres_costs','taxes_ins_costs','misc_costs','actual_loss', 'modcost']:\n",
    "        df = df.withColumn(c,col(c).cast(\"double\"))\n",
    "    for x in ['id_loan','svcg_cycle','delq_sts','repch_flag','flag_mod', 'cd_zero_bal']:\n",
    "        df = df.withColumn(x,col(x).cast(\"String\"))\n",
    "    #     df[ = df[['flag_sc','flag_fthb','cnt_borr','occpy_sts','channel','ppmt_pnlty','zipcode','servicer_name','id_loan','loan_purpose','seller_name']].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time with pyspark : 2.0707733631134033\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "df = spark.read.options(header=False,inferSchema=False).csv(\"SampleInputFiles/sample_svcg_2011.txt\",sep='|',)\n",
    "df = df.toDF('id_loan','svcg_cycle','current_upb','delq_sts','loan_age','mths_remng', 'repch_flag',\n",
    "                  'flag_mod','cd_zero_bal', 'dt_zero_bal','current_int_rt','non_int_brng_upb','dt_lst_pi',\n",
    "                  'mi_recoveries','net_sale_proceeds','non_mi_recoveries','expenses', 'legal_costs', \n",
    "                  'maint_pres_costs','taxes_ins_costs','misc_costs','actual_loss', 'modcost','stepmod_ind')\n",
    "df = fillNA(df)\n",
    "df = changedtype(df)\n",
    "\n",
    "# df.toPandas().to_csv('new.csv', mode='a', header=True,index=False,encoding='utf-8')\n",
    "t2 = time.time()\n",
    "print(\"Total time with pyspark : {}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------+--------+--------+----------+----------+--------+-----------+-----------+--------------+----------------+---------+-------------+-----------------+-----------------+--------+-----------+----------------+---------------+----------+-----------+-------+-----------+\n",
      "|     id_loan|svcg_cycle|current_upb|delq_sts|loan_age|mths_remng|repch_flag|flag_mod|cd_zero_bal|dt_zero_bal|current_int_rt|non_int_brng_upb|dt_lst_pi|mi_recoveries|net_sale_proceeds|non_mi_recoveries|expenses|legal_costs|maint_pres_costs|taxes_ins_costs|misc_costs|actual_loss|modcost|stepmod_ind|\n",
      "+------------+----------+-----------+--------+--------+----------+----------+--------+-----------+-----------+--------------+----------------+---------+-------------+-----------------+-----------------+--------+-----------+----------------+---------------+----------+-----------+-------+-----------+\n",
      "|F111Q1000019|    201103|   123000.0|       0|     0.0|     360.0|         X|       X|         00|     189901|          4.75|             0.0|   189901|          0.0|              0.0|              0.0|     0.0|        0.0|             0.0|            0.0|       0.0|        0.0|    0.0|          X|\n",
      "|F111Q1000019|    201104|   123000.0|       0|     1.0|     359.0|         X|       X|         00|     189901|          4.75|             0.0|   189901|          0.0|              0.0|              0.0|     0.0|        0.0|             0.0|            0.0|       0.0|        0.0|    0.0|          X|\n",
      "|F111Q1000019|    201105|   122000.0|       0|     2.0|     358.0|         X|       X|         00|     189901|          4.75|             0.0|   189901|          0.0|              0.0|              0.0|     0.0|        0.0|             0.0|            0.0|       0.0|        0.0|    0.0|          X|\n",
      "|F111Q1000019|    201106|   122000.0|       0|     3.0|     357.0|         X|       X|         00|     189901|          4.75|             0.0|   189901|          0.0|              0.0|              0.0|     0.0|        0.0|             0.0|            0.0|       0.0|        0.0|    0.0|          X|\n",
      "|F111Q1000019|    201107|   122000.0|       0|     4.0|     356.0|         X|       X|         00|     189901|          4.75|             0.0|   189901|          0.0|              0.0|              0.0|     0.0|        0.0|             0.0|            0.0|       0.0|        0.0|    0.0|          X|\n",
      "+------------+----------+-----------+--------+--------+----------+----------+--------+-----------+-----------+--------------+----------------+---------+-------------+-----------------+-----------------+--------+-----------+----------------+---------------+----------+-----------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNA(df):\n",
    "    df['delq_sts'] = df['delq_sts'].fillna('XX')\n",
    "    df['loan_age'] = df['loan_age'].fillna(999)\n",
    "    df['mths_remng'] = df['mths_remng'].fillna('XX')\n",
    "    df['repch_flag']=df['repch_flag'].fillna('X')\n",
    "    df['flag_mod']=df['flag_mod'].fillna('X')\n",
    "    df['cd_zero_bal']=df['cd_zero_bal'].fillna('00')\n",
    "    df['dt_zero_bal']=df['dt_zero_bal'].fillna('189901')\n",
    "    df['non_int_brng_upb']=df['non_int_brng_upb'].fillna(0)\n",
    "    df['dt_lst_pi']=df['dt_lst_pi'].fillna('189901')\n",
    "    df['mi_recoveries']=df['mi_recoveries'].fillna(0)\n",
    "    df['net_sale_proceeds']=df['net_sale_proceeds'].fillna('0')\n",
    "    df['non_mi_recoveries']=df['non_mi_recoveries'].fillna(0)\n",
    "    df['expenses']=df['expenses'].fillna(0)\n",
    "    df['legal_costs']=df['legal_costs'].fillna(0)\n",
    "    df['maint_pres_costs']=df['maint_pres_costs'].fillna(0)\n",
    "    df['taxes_ins_costs']=df['taxes_ins_costs'].fillna(0)\n",
    "    df['misc_costs']=df['misc_costs'].fillna(0)\n",
    "    df['actual_loss']=df['actual_loss'].fillna(0)\n",
    "    df['modcost']=df['modcost'].fillna(0)\n",
    "    df['stepmod_ind']=df['stepmod_ind'].fillna('X')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count(),len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=glob.glob(\"SampleInputFiles/sample_orig_*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape , df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"SampleOriginationCombined1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SampleOriginationCombined.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
